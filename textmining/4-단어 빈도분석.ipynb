{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도분석\n",
    "\n",
    "* 전체 문서 또는 문서별 단어 출현빈도\n",
    "* 가장 기본적이지만 쉽고 보편적으로 활용되는 방법\n",
    "    * 본격적인 분석 전 데이터에 대한 이해와 흐름을 살펴보기 위한 기초 분석에 해당\n",
    "* 불용어들은 사전에 제거한 후 단어빈도를 분석해야 함\n",
    "* 워드클라우드로 시각화\n",
    "    * pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 타입은 file -i filename 으로 알 수 있음\n",
    "f = open(\"트럼프취임연설문.txt\", 'r', encoding='iso-8859-1')\n",
    "lines = f.readlines()[0]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위로 토큰화\n",
    "words = lines.lower()\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "tokens = tokenizer.tokenize(words)\n",
    "\n",
    "# 불용어 및 한 글자 제거\n",
    "stop_words = stopwords.words('english')\n",
    "stopped_tokens = [i for i in list((tokens)) if not i in stop_words]\n",
    "stopped_tokens2 = [i for i in stopped_tokens if len(i) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "america     20\n",
       "american    11\n",
       "people      10\n",
       "country      9\n",
       "one          8\n",
       "nation       7\n",
       "every        7\n",
       "great        6\n",
       "never        6\n",
       "back         6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 빈도 카운트\n",
    "pd.Series(stopped_tokens2).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_home = os.environ.get('JAVA_HOME', None)\n",
    "if not java_home:\n",
    "    java_path = 'C:/Program Files/Java/jdk-14.0.1/bin'\n",
    "    os.environ['JAVA_HOME'] = java_path\n",
    "else:\n",
    "    print(java_home)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e24e2ac5ba8106d6de97ff5591a62c68103018b4d61809f9d99e49c5850d440"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
